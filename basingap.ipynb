{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjUYZfUVqlaE",
        "outputId": "c6f27ced-bfb3-47ef-c4e5-215d412514a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BASIN GAP EXPERIMENT\n",
            "Testing how varying the local optimum strength affects convergence\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Testing method: FIXED\n",
            "======================================================================\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=0.5, method=fixed\n",
            "  Local optimum (1,1) reward: 0.617\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: 0.413\n",
            "  Iter  500: Reward=0.613, Actions=(1.15, 1.05)\n",
            "  Iter 1000: Reward=0.619, Actions=(1.15, 1.00)\n",
            "  Iter 1500: Reward=0.614, Actions=(1.16, 1.00)\n",
            "  Iter 2000: Reward=0.619, Actions=(1.14, 1.11)\n",
            "  Iter 2500: Reward=0.620, Actions=(1.18, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.0, method=fixed\n",
            "  Local optimum (1,1) reward: 1.135\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.104\n",
            "  Iter  500: Reward=1.091, Actions=(1.08, 0.98)\n",
            "  Iter 1000: Reward=1.105, Actions=(1.18, 1.01)\n",
            "  Iter 1500: Reward=1.114, Actions=(1.02, 1.02)\n",
            "  Iter 2000: Reward=1.127, Actions=(1.04, 0.96)\n",
            "  Iter 2500: Reward=1.116, Actions=(1.14, 0.92)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.5, method=fixed\n",
            "  Local optimum (1,1) reward: 1.652\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.621\n",
            "  Iter  500: Reward=1.601, Actions=(1.00, 1.00)\n",
            "  Iter 1000: Reward=1.617, Actions=(1.00, 1.03)\n",
            "  Iter 1500: Reward=1.619, Actions=(1.04, 0.91)\n",
            "  Iter 2000: Reward=1.593, Actions=(1.08, 1.05)\n",
            "  Iter 2500: Reward=1.621, Actions=(1.12, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=2.0, method=fixed\n",
            "  Local optimum (1,1) reward: 2.169\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -1.139\n",
            "  Iter  500: Reward=2.082, Actions=(1.09, 1.06)\n",
            "  Iter 1000: Reward=2.120, Actions=(1.00, 0.92)\n",
            "  Iter 1500: Reward=2.137, Actions=(1.10, 0.93)\n",
            "  Iter 2000: Reward=2.104, Actions=(1.07, 0.92)\n",
            "  Iter 2500: Reward=2.138, Actions=(1.00, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "======================================================================\n",
            "Testing method: GREEDY\n",
            "======================================================================\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=0.5, method=greedy\n",
            "  Local optimum (1,1) reward: 0.617\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: 0.413\n",
            "  Iter  500: Reward=0.613, Actions=(1.15, 0.99)\n",
            "  Iter 1000: Reward=0.619, Actions=(1.14, 1.00)\n",
            "  Iter 1500: Reward=0.617, Actions=(1.15, 1.00)\n",
            "  Iter 2000: Reward=0.617, Actions=(1.17, 1.07)\n",
            "  Iter 2500: Reward=0.613, Actions=(1.15, 0.99)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.0, method=greedy\n",
            "  Local optimum (1,1) reward: 1.135\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.104\n",
            "  Iter  500: Reward=1.106, Actions=(1.11, 1.00)\n",
            "  Iter 1000: Reward=1.119, Actions=(1.09, 1.00)\n",
            "  Iter 1500: Reward=1.121, Actions=(1.14, 1.04)\n",
            "  Iter 2000: Reward=1.112, Actions=(1.05, 1.06)\n",
            "  Iter 2500: Reward=1.118, Actions=(1.14, 1.03)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.5, method=greedy\n",
            "  Local optimum (1,1) reward: 1.652\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.621\n",
            "  Iter  500: Reward=1.619, Actions=(1.02, 1.07)\n",
            "  Iter 1000: Reward=1.632, Actions=(1.02, 1.00)\n",
            "  Iter 1500: Reward=1.620, Actions=(0.99, 0.92)\n",
            "  Iter 2000: Reward=1.608, Actions=(1.10, 0.99)\n",
            "  Iter 2500: Reward=1.621, Actions=(1.10, 1.01)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=2.0, method=greedy\n",
            "  Local optimum (1,1) reward: 2.169\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -1.139\n",
            "  Iter  500: Reward=2.128, Actions=(0.94, 0.99)\n",
            "  Iter 1000: Reward=2.117, Actions=(1.03, 1.00)\n",
            "  Iter 1500: Reward=2.124, Actions=(1.02, 1.00)\n",
            "  Iter 2000: Reward=2.092, Actions=(1.08, 1.00)\n",
            "  Iter 2500: Reward=2.128, Actions=(1.07, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "======================================================================\n",
            "Testing method: WEIGHTED\n",
            "======================================================================\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=0.5, method=weighted\n",
            "  Local optimum (1,1) reward: 0.617\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: 0.413\n",
            "  Iter  500: Reward=0.615, Actions=(1.27, 1.00)\n",
            "  Iter 1000: Reward=0.616, Actions=(1.24, 0.93)\n",
            "  Iter 1500: Reward=0.620, Actions=(1.23, 0.94)\n",
            "  Iter 2000: Reward=0.611, Actions=(1.20, 0.90)\n",
            "  Iter 2500: Reward=0.619, Actions=(1.25, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.0, method=weighted\n",
            "  Local optimum (1,1) reward: 1.135\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.104\n",
            "  Iter  500: Reward=1.119, Actions=(1.07, 1.00)\n",
            "  Iter 1000: Reward=1.098, Actions=(1.12, 0.91)\n",
            "  Iter 1500: Reward=1.118, Actions=(1.01, 1.01)\n",
            "  Iter 2000: Reward=1.108, Actions=(1.13, 0.90)\n",
            "  Iter 2500: Reward=1.121, Actions=(1.14, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.5, method=weighted\n",
            "  Local optimum (1,1) reward: 1.652\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.621\n",
            "  Iter  500: Reward=1.618, Actions=(1.14, 1.08)\n",
            "  Iter 1000: Reward=1.607, Actions=(1.01, 1.00)\n",
            "  Iter 1500: Reward=1.613, Actions=(1.11, 0.98)\n",
            "  Iter 2000: Reward=1.627, Actions=(1.04, 1.00)\n",
            "  Iter 2500: Reward=1.624, Actions=(1.04, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=2.0, method=weighted\n",
            "  Local optimum (1,1) reward: 2.169\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -1.139\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "OT-TRPO with Basin Gap Variation\n",
        "Implements the basin gap idea by scaling the local optimum coefficient.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize_scalar\n",
        "import copy\n",
        "import os\n",
        "\n",
        "\n",
        "class DifferentialGameEnv:\n",
        "    \"\"\"\n",
        "    Two-player differential game with adjustable basin gap.\n",
        "    The basin_gap_factor controls the relative height of local vs global optima.\n",
        "\n",
        "    R(a1, a2) = α_g*N(5,5; 1,3) + k*α_ℓ*N(1,1; 1,1) + 0.1*a1\n",
        "\n",
        "    where k is the basin_gap_factor:\n",
        "    - k < 1: Local optimum is less attractive (easier to escape)\n",
        "    - k = 1: Original balance (default)\n",
        "    - k > 1: Local optimum is more attractive (harder to escape)\n",
        "    \"\"\"\n",
        "    def __init__(self, basin_gap_factor=1.0):\n",
        "        self.n_agents = 2\n",
        "        self.basin_gap_factor = basin_gap_factor\n",
        "\n",
        "        # Base coefficients\n",
        "        self.global_coef_base = 10.0 / (2 * np.pi * np.sqrt(9.0))\n",
        "        self.local_coef_base = 6.5 / (2 * np.pi)\n",
        "\n",
        "        # Apply basin gap factor to local coefficient\n",
        "        self.local_coef = self.local_coef_base * basin_gap_factor\n",
        "        self.global_coef = self.global_coef_base\n",
        "\n",
        "    def reward(self, a1, a2):\n",
        "        \"\"\"Compute reward for joint actions.\"\"\"\n",
        "        # Global optimum at (5,5)\n",
        "        global_term = np.exp(-0.5 * ((a1 - 5)**2 / 1.0 + (a2 - 5)**2 / 9.0))\n",
        "\n",
        "        # Local optimum at (1,1) - scaled by basin_gap_factor\n",
        "        local_term = np.exp(-0.5 * ((a1 - 1)**2 / 1.0 + (a2 - 1)**2 / 1.0))\n",
        "\n",
        "        # Linear bias term\n",
        "        linear_term = 0.1 * a1\n",
        "\n",
        "        return self.global_coef * global_term + self.local_coef * local_term + linear_term\n",
        "\n",
        "    def get_reward_at_optima(self):\n",
        "        \"\"\"Return rewards at local and global optima for comparison.\"\"\"\n",
        "        local_reward = self.reward(1.0, 1.0)\n",
        "        global_reward = self.reward(5.0, 5.0)\n",
        "        return {\n",
        "            'local': local_reward,\n",
        "            'global': global_reward,\n",
        "            'gap': global_reward - local_reward,\n",
        "            'ratio': global_reward / local_reward if local_reward > 0 else np.inf\n",
        "        }\n",
        "\n",
        "\n",
        "class GaussianPolicy:\n",
        "    \"\"\"Simple Gaussian policy with mean and std parameters.\"\"\"\n",
        "    def __init__(self, mean=1.0, std=1.15):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.mean_history = [mean]\n",
        "        self.std_history = [std]\n",
        "\n",
        "    def sample(self, n_samples=1):\n",
        "        \"\"\"Sample actions from the policy.\"\"\"\n",
        "        samples = np.random.normal(self.mean, self.std, n_samples)\n",
        "        return np.clip(samples, 0, 7)\n",
        "\n",
        "    def wasserstein_distance(self, other_policy):\n",
        "        \"\"\"1-Wasserstein distance for 1D Gaussians.\"\"\"\n",
        "        return np.abs(self.mean - other_policy.mean) + np.abs(self.std - other_policy.std)\n",
        "\n",
        "    def update(self, new_mean, new_std):\n",
        "        \"\"\"Update policy parameters and record history.\"\"\"\n",
        "        self.mean = new_mean\n",
        "        self.std = new_std\n",
        "        self.mean_history.append(new_mean)\n",
        "        self.std_history.append(new_std)\n",
        "\n",
        "\n",
        "class SimpleCritic:\n",
        "    \"\"\"A simple baseline critic using an exponential moving average of rewards.\"\"\"\n",
        "    def __init__(self, lr=0.2):\n",
        "        self.lr = lr\n",
        "        self.baseline = 0.0\n",
        "\n",
        "    def update(self, rewards):\n",
        "        \"\"\"Update the baseline.\"\"\"\n",
        "        if len(rewards) > 0:\n",
        "            self.baseline = (1 - self.lr) * self.baseline + self.lr * np.mean(rewards)\n",
        "\n",
        "    def get_baseline(self):\n",
        "        return self.baseline\n",
        "\n",
        "\n",
        "class OTTRPO:\n",
        "    \"\"\"OT-TRPO with selectable adaptive trust region method.\"\"\"\n",
        "    def __init__(self, env, epsilon=0.1, batch_size=30, critic_lr=0.2,\n",
        "                 n_iterations=4000, initial_mean=1.5, initial_std=0.5,\n",
        "                 transport_cost_type='l2', adaptive_method='fixed', caatr_C=0.02):\n",
        "        self.env = env\n",
        "        self.n_agents = env.n_agents\n",
        "        self.epsilon_total = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.n_iterations = n_iterations\n",
        "        self.transport_cost_type = transport_cost_type\n",
        "        self.adaptive_method = adaptive_method\n",
        "        self.caatr_C = caatr_C\n",
        "\n",
        "        self.policies = [GaussianPolicy(mean=initial_mean, std=initial_std)\n",
        "                        for _ in range(self.n_agents)]\n",
        "        self.critics = [SimpleCritic(lr=critic_lr) for _ in range(self.n_agents)]\n",
        "\n",
        "        # History Tracking\n",
        "        self.reward_history = []\n",
        "        self.trajectory = {'a1': [initial_mean], 'a2': [initial_mean]}\n",
        "        self.lambda_history = [[] for _ in range(self.n_agents)]\n",
        "        self.wasserstein_history = [[0.0] for _ in range(self.n_agents)]\n",
        "        self.epsilon_history = [[epsilon] for _ in range(self.n_agents)]\n",
        "\n",
        "        # Track convergence\n",
        "        self.converged_to_global = False\n",
        "        self.convergence_iteration = None\n",
        "\n",
        "    def _get_adaptive_epsilons(self, batch):\n",
        "        \"\"\"Calculate trust region radius for each agent based on selected method.\"\"\"\n",
        "        if self.adaptive_method == 'fixed':\n",
        "            return [self.epsilon_total] * self.n_agents\n",
        "\n",
        "        advantages = [self.compute_advantages(batch, i) for i in range(self.n_agents)]\n",
        "        avg_advantages = np.array([np.mean(adv) for adv in advantages])\n",
        "\n",
        "        if self.adaptive_method == 'greedy':\n",
        "            last_w_dist = np.array([self.wasserstein_history[i][-1]\n",
        "                                    for i in range(self.n_agents)])\n",
        "            scores = np.abs(avg_advantages) / (last_w_dist + 1e-8)\n",
        "            scores[np.isnan(scores)] = 0\n",
        "            if np.sum(scores) < 1e-8:\n",
        "                return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "            normalized_scores = scores / np.sum(scores)\n",
        "            return normalized_scores * self.epsilon_total\n",
        "\n",
        "        if self.adaptive_method == 'weighted':\n",
        "            utilities = np.maximum(0, avg_advantages)\n",
        "            if np.sum(utilities) < 1e-8:\n",
        "                return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "\n",
        "            lambda_val = np.max(utilities) + 1e-6\n",
        "            for _ in range(10):\n",
        "                allocations = np.maximum(0, utilities / lambda_val - 1e-4)\n",
        "                current_total = np.sum(allocations)\n",
        "                if abs(current_total - self.epsilon_total) < 1e-5:\n",
        "                    break\n",
        "                if current_total < 1e-8:\n",
        "                    lambda_val *= 0.5\n",
        "                else:\n",
        "                    lambda_val *= (current_total / self.epsilon_total)\n",
        "\n",
        "            final_allocations = np.maximum(0, utilities / lambda_val - 1e-4)\n",
        "            if np.sum(final_allocations) > 1e-8:\n",
        "                return final_allocations / np.sum(final_allocations) * self.epsilon_total\n",
        "            return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "\n",
        "        if self.adaptive_method == 'caatr':\n",
        "            if len(self.wasserstein_history[0]) < 2:\n",
        "                return [self.epsilon_total] * self.n_agents\n",
        "\n",
        "            epsilons = []\n",
        "            for i in range(self.n_agents):\n",
        "                teammate_drift = sum([self.wasserstein_history[j][-1]\n",
        "                                     for j in range(self.n_agents) if i != j])\n",
        "                eps = self.caatr_C / (teammate_drift + 1e-8)\n",
        "                epsilons.append(eps)\n",
        "            return epsilons\n",
        "\n",
        "        raise ValueError(f\"Unknown adaptive_method: {self.adaptive_method}\")\n",
        "\n",
        "    def transport_cost(self, a1, a2):\n",
        "        return (a1 - a2)**2 if self.transport_cost_type == 'l2' else np.abs(a1 - a2)\n",
        "\n",
        "    def collect_batch(self):\n",
        "        batch = {'actions': [[] for _ in range(self.n_agents)], 'rewards': []}\n",
        "        for _ in range(self.batch_size):\n",
        "            actions = [p.sample() for p in self.policies]\n",
        "            reward = self.env.reward(*actions)\n",
        "            for i in range(self.n_agents):\n",
        "                batch['actions'][i].append(actions[i])\n",
        "            batch['rewards'].append(reward)\n",
        "        return batch\n",
        "\n",
        "    def compute_advantages(self, batch, agent_id):\n",
        "        rewards = np.array(batch['rewards'])\n",
        "        baseline = self.critics[agent_id].get_baseline()\n",
        "        advantages = rewards - baseline\n",
        "        if np.std(advantages) > 1e-8:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
        "        return advantages\n",
        "\n",
        "    def solve_dual_problem(self, agent_id, batch, old_policy, epsilon_i):\n",
        "        actions = np.array(batch['actions'][agent_id])\n",
        "        advantages = self.compute_advantages(batch, agent_id)\n",
        "\n",
        "        def advantage_function(a_prime):\n",
        "            if len(actions) == 0:\n",
        "                return 0.0\n",
        "            weights = np.exp(-0.5 * ((actions - a_prime) / old_policy.std)**2)\n",
        "            weights /= (np.sum(weights) + 1e-10)\n",
        "            return np.sum(weights * advantages)\n",
        "\n",
        "        def dual_objective(lambda_val):\n",
        "            if lambda_val < 0:\n",
        "                return 1e10\n",
        "            inner_values = []\n",
        "            for a_old in actions:\n",
        "                def inner_obj(a_prime):\n",
        "                    return -(advantage_function(a_prime) -\n",
        "                            lambda_val * self.transport_cost(a_old, a_prime))\n",
        "                res = minimize_scalar(inner_obj, bounds=(0, 7), method='bounded')\n",
        "                inner_values.append(-res.fun)\n",
        "            return lambda_val * epsilon_i + np.mean(inner_values)\n",
        "\n",
        "        res = minimize_scalar(dual_objective, bounds=(0, 20.0), method='bounded')\n",
        "        optimal_lambda = res.x\n",
        "\n",
        "        action_grid = np.linspace(0, 7, 50)\n",
        "        policy_weights = np.zeros_like(action_grid)\n",
        "        for i, a_prime in enumerate(action_grid):\n",
        "            values = [advantage_function(a_prime) -\n",
        "                     optimal_lambda * self.transport_cost(a_old, a_prime)\n",
        "                     for a_old in actions]\n",
        "            policy_weights[i] = np.mean(values)\n",
        "\n",
        "        policy_weights = np.exp(policy_weights / (old_policy.std**2 + 1e-8))\n",
        "        policy_weights /= np.sum(policy_weights)\n",
        "\n",
        "        new_mean = np.sum(action_grid * policy_weights)\n",
        "        new_var = np.sum(((action_grid - new_mean)**2) * policy_weights)\n",
        "        new_std = np.sqrt(new_var) if new_var > 0.01 else old_policy.std\n",
        "        return np.clip(new_mean, 0, 7), np.clip(new_std, 0.1, 3.0), optimal_lambda\n",
        "\n",
        "    def update_agent_dual(self, agent_id, batch, epsilon_i):\n",
        "        old_policy = copy.deepcopy(self.policies[agent_id])\n",
        "        try:\n",
        "            new_mean, new_std, opt_lambda = self.solve_dual_problem(\n",
        "                agent_id, batch, old_policy, epsilon_i)\n",
        "\n",
        "            temp_policy = GaussianPolicy(new_mean, new_std)\n",
        "            w_dist = old_policy.wasserstein_distance(temp_policy)\n",
        "\n",
        "            if w_dist > epsilon_i:\n",
        "                alpha = epsilon_i / (w_dist + 1e-8)\n",
        "                new_mean = old_policy.mean + alpha * (new_mean - old_policy.mean)\n",
        "                new_std = old_policy.std + alpha * (new_std - old_policy.std)\n",
        "\n",
        "            self.policies[agent_id].update(new_mean, new_std)\n",
        "            final_w_dist = old_policy.wasserstein_distance(self.policies[agent_id])\n",
        "\n",
        "            self.lambda_history[agent_id].append(opt_lambda)\n",
        "            self.wasserstein_history[agent_id].append(final_w_dist)\n",
        "            self.epsilon_history[agent_id].append(epsilon_i)\n",
        "        except Exception as e:\n",
        "            self.policies[agent_id].update(old_policy.mean, old_policy.std)\n",
        "            self.lambda_history[agent_id].append(0.0)\n",
        "            self.wasserstein_history[agent_id].append(0.0)\n",
        "            self.epsilon_history[agent_id].append(epsilon_i)\n",
        "\n",
        "    def check_convergence(self, iteration):\n",
        "        \"\"\"Check if agents have converged to global optimum.\"\"\"\n",
        "        a1, a2 = self.policies[0].mean, self.policies[1].mean\n",
        "        distance_to_global = np.sqrt((a1 - 5.0)**2 + (a2 - 5.0)**2)\n",
        "\n",
        "        if distance_to_global < 0.5 and not self.converged_to_global:\n",
        "            self.converged_to_global = True\n",
        "            self.convergence_iteration = iteration\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train(self):\n",
        "        basin_factor = self.env.basin_gap_factor\n",
        "        print(f\"\\nTraining OT-TRPO with basin_gap_factor={basin_factor:.1f}, method={self.adaptive_method}\")\n",
        "\n",
        "        # Print reward structure info\n",
        "        optima_info = self.env.get_reward_at_optima()\n",
        "        print(f\"  Local optimum (1,1) reward: {optima_info['local']:.3f}\")\n",
        "        print(f\"  Global optimum (5,5) reward: {optima_info['global']:.3f}\")\n",
        "        print(f\"  Reward gap: {optima_info['gap']:.3f}\")\n",
        "\n",
        "        for iteration in range(self.n_iterations):\n",
        "            batch = self.collect_batch()\n",
        "            for critic in self.critics:\n",
        "                critic.update(batch['rewards'])\n",
        "\n",
        "            adaptive_epsilons = self._get_adaptive_epsilons(batch)\n",
        "\n",
        "            for agent_id in range(self.n_agents):\n",
        "                self.update_agent_dual(agent_id, batch, adaptive_epsilons[agent_id])\n",
        "\n",
        "            avg_reward = np.mean(batch['rewards'])\n",
        "            self.reward_history.append(avg_reward)\n",
        "            self.trajectory['a1'].append(self.policies[0].mean)\n",
        "            self.trajectory['a2'].append(self.policies[1].mean)\n",
        "\n",
        "            # Check convergence\n",
        "            if self.check_convergence(iteration):\n",
        "                print(f\"  *** Converged to global optimum at iteration {iteration}! ***\")\n",
        "\n",
        "            if iteration > 0 and iteration % 500 == 0:\n",
        "                print(f\"  Iter {iteration:4d}: Reward={avg_reward:.3f}, \"\n",
        "                     f\"Actions=({self.policies[0].mean:.2f}, {self.policies[1].mean:.2f})\")\n",
        "\n",
        "        # Final status\n",
        "        if self.converged_to_global:\n",
        "            print(f\"  Final: Converged to GLOBAL optimum (iter {self.convergence_iteration})\")\n",
        "        else:\n",
        "            a1, a2 = self.policies[0].mean, self.policies[1].mean\n",
        "            if np.sqrt((a1 - 1.0)**2 + (a2 - 1.0)**2) < 0.5:\n",
        "                print(f\"  Final: Stuck at LOCAL optimum\")\n",
        "            else:\n",
        "                print(f\"  Final: Neither optimum reached\")\n",
        "\n",
        "\n",
        "def run_basin_gap_experiment():\n",
        "    \"\"\"Run experiments with different basin gap factors.\"\"\"\n",
        "\n",
        "    # Basin gap factors to test\n",
        "    basin_factors = [0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "    # Methods to test\n",
        "    methods = ['fixed', 'greedy', 'weighted', 'caatr']\n",
        "\n",
        "    # Store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"BASIN GAP EXPERIMENT\")\n",
        "    print(\"Testing how varying the local optimum strength affects convergence\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for method in methods:\n",
        "        results[method] = {}\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Testing method: {method.upper()}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for k in basin_factors:\n",
        "            env = DifferentialGameEnv(basin_gap_factor=k)\n",
        "\n",
        "            # Set appropriate epsilon for each method\n",
        "            if method in ['greedy', 'weighted']:\n",
        "                epsilon = 0.2\n",
        "            else:\n",
        "                epsilon = 0.1\n",
        "\n",
        "            ottrpo = OTTRPO(\n",
        "                env,\n",
        "                epsilon=epsilon,\n",
        "                batch_size=30,\n",
        "                critic_lr=0.2,\n",
        "                n_iterations=3000,\n",
        "                initial_mean=1.5,\n",
        "                initial_std=0.5,\n",
        "                adaptive_method=method,\n",
        "                caatr_C=0.02 if method == 'caatr' else 0.02\n",
        "            )\n",
        "\n",
        "            ottrpo.train()\n",
        "\n",
        "            # Store results\n",
        "            results[method][k] = {\n",
        "                'converged_to_global': ottrpo.converged_to_global,\n",
        "                'convergence_iter': ottrpo.convergence_iteration,\n",
        "                'final_actions': (ottrpo.policies[0].mean, ottrpo.policies[1].mean),\n",
        "                'final_reward': ottrpo.reward_history[-1],\n",
        "                'trajectory': copy.deepcopy(ottrpo.trajectory),\n",
        "                'rewards': copy.deepcopy(ottrpo.reward_history)\n",
        "            }\n",
        "\n",
        "    # Create comparison plots\n",
        "    create_basin_gap_plots(results, basin_factors, methods)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def create_basin_gap_plots(results, basin_factors, methods):\n",
        "    \"\"\"Create visualization comparing different basin gap factors.\"\"\"\n",
        "\n",
        "    # Plot 1: Convergence success rate\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    for idx, method in enumerate(methods):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "        # Plot reward curves for different k values\n",
        "        for k in basin_factors:\n",
        "            rewards = results[method][k]['rewards']\n",
        "            label = f\"k={k:.1f}\"\n",
        "            if results[method][k]['converged_to_global']:\n",
        "                label += \" ✓\"\n",
        "            ax.plot(rewards, label=label, alpha=0.7, linewidth=2)\n",
        "\n",
        "        ax.set_title(f'{method.upper()} Method - Basin Gap Effect')\n",
        "        ax.set_xlabel('Iteration')\n",
        "        ax.set_ylabel('Average Reward')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add horizontal lines for optima\n",
        "        env_test = DifferentialGameEnv(basin_gap_factor=1.0)\n",
        "        global_reward = env_test.reward(5.0, 5.0)\n",
        "        ax.axhline(y=global_reward, color='g', linestyle='--', alpha=0.5, label='Global')\n",
        "\n",
        "    plt.suptitle('Basin Gap Experiment: Effect of Local Optimum Strength', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    plt.savefig(os.path.join('results', 'basin_gap_comparison.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Summary statistics\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Convergence success\n",
        "    ax = axes[0]\n",
        "    x = np.arange(len(basin_factors))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, method in enumerate(methods):\n",
        "        success = [1 if results[method][k]['converged_to_global'] else 0\n",
        "                  for k in basin_factors]\n",
        "        ax.bar(x + i*width, success, width, label=method.upper())\n",
        "\n",
        "    ax.set_xlabel('Basin Gap Factor (k)')\n",
        "    ax.set_ylabel('Converged to Global (1=Yes, 0=No)')\n",
        "    ax.set_title('Convergence Success by Basin Gap')\n",
        "    ax.set_xticks(x + width * 1.5)\n",
        "    ax.set_xticklabels([f\"{k:.1f}\" for k in basin_factors])\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Final rewards\n",
        "    ax = axes[1]\n",
        "    for i, method in enumerate(methods):\n",
        "        final_rewards = [results[method][k]['final_reward'] for k in basin_factors]\n",
        "        ax.plot(basin_factors, final_rewards, marker='o', label=method.upper(), linewidth=2)\n",
        "\n",
        "    ax.set_xlabel('Basin Gap Factor (k)')\n",
        "    ax.set_ylabel('Final Average Reward')\n",
        "    ax.set_title('Final Performance by Basin Gap')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('results', 'basin_gap_summary.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BASIN GAP EXPERIMENT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nConvergence to Global Optimum:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{'Method':<12} | \" + \" | \".join([f\"k={k:.1f}\" for k in basin_factors]))\n",
        "    print(\"-\" * 40)\n",
        "    for method in methods:\n",
        "        row = f\"{method.upper():<12} | \"\n",
        "        for k in basin_factors:\n",
        "            if results[method][k]['converged_to_global']:\n",
        "                row += \" ✓   | \"\n",
        "            else:\n",
        "                row += \" ✗   | \"\n",
        "        print(row)\n",
        "\n",
        "    print(\"\\nKey Insights:\")\n",
        "    print(\"- k < 1.0: Weakens local optimum, easier to reach global\")\n",
        "    print(\"- k = 1.0: Original balance between optima\")\n",
        "    print(\"- k > 1.0: Strengthens local optimum, harder to escape\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the basin gap experiment\n",
        "    results = run_basin_gap_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "feTFYAk4RsQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90633db6-c8c1-4ed5-9f40-3a042264d53a",
        "id": "zzXibun-RsXU"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BASIN GAP EXPERIMENT\n",
            "Testing how varying the local optimum strength affects convergence\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Testing method: FIXED\n",
            "======================================================================\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=0.5, method=fixed\n",
            "  Local optimum (1,1) reward: 0.617\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: 0.413\n",
            "  Iter  500: Reward=0.613, Actions=(1.15, 1.05)\n",
            "  Iter 1000: Reward=0.619, Actions=(1.15, 1.00)\n",
            "  Iter 1500: Reward=0.614, Actions=(1.16, 1.00)\n",
            "  Iter 2000: Reward=0.619, Actions=(1.14, 1.11)\n",
            "  Iter 2500: Reward=0.620, Actions=(1.18, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.0, method=fixed\n",
            "  Local optimum (1,1) reward: 1.135\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.104\n",
            "  Iter  500: Reward=1.091, Actions=(1.08, 0.98)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "OT-TRPO with Basin Gap Variation\n",
        "Implements the basin gap idea by scaling the local optimum coefficient.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize_scalar\n",
        "import copy\n",
        "import os\n",
        "\n",
        "\n",
        "class DifferentialGameEnv:\n",
        "    \"\"\"\n",
        "    Two-player differential game with adjustable basin gap.\n",
        "    The basin_gap_factor controls the relative height of local vs global optima.\n",
        "\n",
        "    R(a1, a2) = α_g*N(5,5; 1,3) + k*α_ℓ*N(1,1; 1,1) + 0.1*a1\n",
        "\n",
        "    where k is the basin_gap_factor:\n",
        "    - k < 1: Local optimum is less attractive (easier to escape)\n",
        "    - k = 1: Original balance (default)\n",
        "    - k > 1: Local optimum is more attractive (harder to escape)\n",
        "    \"\"\"\n",
        "    def __init__(self, basin_gap_factor=1.0):\n",
        "        self.n_agents = 2\n",
        "        self.basin_gap_factor = basin_gap_factor\n",
        "\n",
        "        # Base coefficients\n",
        "        self.global_coef_base = 10.0 / (2 * np.pi * np.sqrt(9.0))\n",
        "        self.local_coef_base = 6.5 / (2 * np.pi)\n",
        "\n",
        "        # Apply basin gap factor to local coefficient\n",
        "        self.local_coef = self.local_coef_base * basin_gap_factor\n",
        "        self.global_coef = self.global_coef_base\n",
        "\n",
        "    def reward(self, a1, a2):\n",
        "        \"\"\"Compute reward for joint actions.\"\"\"\n",
        "        # Global optimum at (5,5)\n",
        "        global_term = np.exp(-0.5 * ((a1 - 5)**2 / 1.0 + (a2 - 5)**2 / 9.0))\n",
        "\n",
        "        # Local optimum at (1,1) - scaled by basin_gap_factor\n",
        "        local_term = np.exp(-0.5 * ((a1 - 1)**2 / 1.0 + (a2 - 1)**2 / 1.0))\n",
        "\n",
        "        # Linear bias term\n",
        "        linear_term = 0.1 * a1\n",
        "\n",
        "        return self.global_coef * global_term + self.local_coef * local_term + linear_term\n",
        "\n",
        "    def get_reward_at_optima(self):\n",
        "        \"\"\"Return rewards at local and global optima for comparison.\"\"\"\n",
        "        local_reward = self.reward(1.0, 1.0)\n",
        "        global_reward = self.reward(5.0, 5.0)\n",
        "        return {\n",
        "            'local': local_reward,\n",
        "            'global': global_reward,\n",
        "            'gap': global_reward - local_reward,\n",
        "            'ratio': global_reward / local_reward if local_reward > 0 else np.inf\n",
        "        }\n",
        "\n",
        "\n",
        "class GaussianPolicy:\n",
        "    \"\"\"Simple Gaussian policy with mean and std parameters.\"\"\"\n",
        "    def __init__(self, mean=1.0, std=1.15):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.mean_history = [mean]\n",
        "        self.std_history = [std]\n",
        "\n",
        "    def sample(self, n_samples=1):\n",
        "        \"\"\"Sample actions from the policy.\"\"\"\n",
        "        samples = np.random.normal(self.mean, self.std, n_samples)\n",
        "        return np.clip(samples, 0, 7)\n",
        "\n",
        "    def wasserstein_distance(self, other_policy):\n",
        "        \"\"\"1-Wasserstein distance for 1D Gaussians.\"\"\"\n",
        "        return np.abs(self.mean - other_policy.mean) + np.abs(self.std - other_policy.std)\n",
        "\n",
        "    def update(self, new_mean, new_std):\n",
        "        \"\"\"Update policy parameters and record history.\"\"\"\n",
        "        self.mean = new_mean\n",
        "        self.std = new_std\n",
        "        self.mean_history.append(new_mean)\n",
        "        self.std_history.append(new_std)\n",
        "\n",
        "\n",
        "class SimpleCritic:\n",
        "    \"\"\"A simple baseline critic using an exponential moving average of rewards.\"\"\"\n",
        "    def __init__(self, lr=0.2):\n",
        "        self.lr = lr\n",
        "        self.baseline = 0.0\n",
        "\n",
        "    def update(self, rewards):\n",
        "        \"\"\"Update the baseline.\"\"\"\n",
        "        if len(rewards) > 0:\n",
        "            self.baseline = (1 - self.lr) * self.baseline + self.lr * np.mean(rewards)\n",
        "\n",
        "    def get_baseline(self):\n",
        "        return self.baseline\n",
        "\n",
        "\n",
        "class OTTRPO:\n",
        "    \"\"\"OT-TRPO with selectable adaptive trust region method.\"\"\"\n",
        "    def __init__(self, env, epsilon=0.1, batch_size=30, critic_lr=0.2,\n",
        "                 n_iterations=4000, initial_mean=1.5, initial_std=0.5,\n",
        "                 transport_cost_type='l2', adaptive_method='fixed', caatr_C=0.02):\n",
        "        self.env = env\n",
        "        self.n_agents = env.n_agents\n",
        "        self.epsilon_total = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.n_iterations = n_iterations\n",
        "        self.transport_cost_type = transport_cost_type\n",
        "        self.adaptive_method = adaptive_method\n",
        "        self.caatr_C = caatr_C\n",
        "\n",
        "        self.policies = [GaussianPolicy(mean=initial_mean, std=initial_std)\n",
        "                        for _ in range(self.n_agents)]\n",
        "        self.critics = [SimpleCritic(lr=critic_lr) for _ in range(self.n_agents)]\n",
        "\n",
        "        # History Tracking\n",
        "        self.reward_history = []\n",
        "        self.trajectory = {'a1': [initial_mean], 'a2': [initial_mean]}\n",
        "        self.lambda_history = [[] for _ in range(self.n_agents)]\n",
        "        self.wasserstein_history = [[0.0] for _ in range(self.n_agents)]\n",
        "        self.epsilon_history = [[epsilon] for _ in range(self.n_agents)]\n",
        "\n",
        "        # Track convergence\n",
        "        self.converged_to_global = False\n",
        "        self.convergence_iteration = None\n",
        "\n",
        "    def _get_adaptive_epsilons(self, batch):\n",
        "        \"\"\"Calculate trust region radius for each agent based on selected method.\"\"\"\n",
        "        if self.adaptive_method == 'fixed':\n",
        "            return [self.epsilon_total] * self.n_agents\n",
        "\n",
        "        advantages = [self.compute_advantages(batch, i) for i in range(self.n_agents)]\n",
        "        avg_advantages = np.array([np.mean(adv) for adv in advantages])\n",
        "\n",
        "        if self.adaptive_method == 'greedy':\n",
        "            last_w_dist = np.array([self.wasserstein_history[i][-1]\n",
        "                                    for i in range(self.n_agents)])\n",
        "            scores = np.abs(avg_advantages) / (last_w_dist + 1e-8)\n",
        "            scores[np.isnan(scores)] = 0\n",
        "            if np.sum(scores) < 1e-8:\n",
        "                return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "            normalized_scores = scores / np.sum(scores)\n",
        "            return normalized_scores * self.epsilon_total\n",
        "\n",
        "        if self.adaptive_method == 'weighted':\n",
        "            utilities = np.maximum(0, avg_advantages)\n",
        "            if np.sum(utilities) < 1e-8:\n",
        "                return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "\n",
        "            lambda_val = np.max(utilities) + 1e-6\n",
        "            for _ in range(10):\n",
        "                allocations = np.maximum(0, utilities / lambda_val - 1e-4)\n",
        "                current_total = np.sum(allocations)\n",
        "                if abs(current_total - self.epsilon_total) < 1e-5:\n",
        "                    break\n",
        "                if current_total < 1e-8:\n",
        "                    lambda_val *= 0.5\n",
        "                else:\n",
        "                    lambda_val *= (current_total / self.epsilon_total)\n",
        "\n",
        "            final_allocations = np.maximum(0, utilities / lambda_val - 1e-4)\n",
        "            if np.sum(final_allocations) > 1e-8:\n",
        "                return final_allocations / np.sum(final_allocations) * self.epsilon_total\n",
        "            return [self.epsilon_total / self.n_agents] * self.n_agents\n",
        "\n",
        "        if self.adaptive_method == 'caatr':\n",
        "            if len(self.wasserstein_history[0]) < 2:\n",
        "                return [self.epsilon_total] * self.n_agents\n",
        "\n",
        "            epsilons = []\n",
        "            for i in range(self.n_agents):\n",
        "                teammate_drift = sum([self.wasserstein_history[j][-1]\n",
        "                                     for j in range(self.n_agents) if i != j])\n",
        "                eps = self.caatr_C / (teammate_drift + 1e-8)\n",
        "                epsilons.append(eps)\n",
        "            return epsilons\n",
        "\n",
        "        raise ValueError(f\"Unknown adaptive_method: {self.adaptive_method}\")\n",
        "\n",
        "    def transport_cost(self, a1, a2):\n",
        "        return (a1 - a2)**2 if self.transport_cost_type == 'l2' else np.abs(a1 - a2)\n",
        "\n",
        "    def collect_batch(self):\n",
        "        batch = {'actions': [[] for _ in range(self.n_agents)], 'rewards': []}\n",
        "        for _ in range(self.batch_size):\n",
        "            actions = [p.sample() for p in self.policies]\n",
        "            reward = self.env.reward(*actions)\n",
        "            for i in range(self.n_agents):\n",
        "                batch['actions'][i].append(actions[i])\n",
        "            batch['rewards'].append(reward)\n",
        "        return batch\n",
        "\n",
        "    def compute_advantages(self, batch, agent_id):\n",
        "        rewards = np.array(batch['rewards'])\n",
        "        baseline = self.critics[agent_id].get_baseline()\n",
        "        advantages = rewards - baseline\n",
        "        if np.std(advantages) > 1e-8:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
        "        return advantages\n",
        "\n",
        "    def solve_dual_problem(self, agent_id, batch, old_policy, epsilon_i):\n",
        "        actions = np.array(batch['actions'][agent_id])\n",
        "        advantages = self.compute_advantages(batch, agent_id)\n",
        "\n",
        "        def advantage_function(a_prime):\n",
        "            if len(actions) == 0:\n",
        "                return 0.0\n",
        "            weights = np.exp(-0.5 * ((actions - a_prime) / old_policy.std)**2)\n",
        "            weights /= (np.sum(weights) + 1e-10)\n",
        "            return np.sum(weights * advantages)\n",
        "\n",
        "        def dual_objective(lambda_val):\n",
        "            if lambda_val < 0:\n",
        "                return 1e10\n",
        "            inner_values = []\n",
        "            for a_old in actions:\n",
        "                def inner_obj(a_prime):\n",
        "                    return -(advantage_function(a_prime) -\n",
        "                            lambda_val * self.transport_cost(a_old, a_prime))\n",
        "                res = minimize_scalar(inner_obj, bounds=(0, 7), method='bounded')\n",
        "                inner_values.append(-res.fun)\n",
        "            return lambda_val * epsilon_i + np.mean(inner_values)\n",
        "\n",
        "        res = minimize_scalar(dual_objective, bounds=(0, 20.0), method='bounded')\n",
        "        optimal_lambda = res.x\n",
        "\n",
        "        action_grid = np.linspace(0, 7, 50)\n",
        "        policy_weights = np.zeros_like(action_grid)\n",
        "        for i, a_prime in enumerate(action_grid):\n",
        "            values = [advantage_function(a_prime) -\n",
        "                     optimal_lambda * self.transport_cost(a_old, a_prime)\n",
        "                     for a_old in actions]\n",
        "            policy_weights[i] = np.mean(values)\n",
        "\n",
        "        policy_weights = np.exp(policy_weights / (old_policy.std**2 + 1e-8))\n",
        "        policy_weights /= np.sum(policy_weights)\n",
        "\n",
        "        new_mean = np.sum(action_grid * policy_weights)\n",
        "        new_var = np.sum(((action_grid - new_mean)**2) * policy_weights)\n",
        "        new_std = np.sqrt(new_var) if new_var > 0.01 else old_policy.std\n",
        "        return np.clip(new_mean, 0, 7), np.clip(new_std, 0.1, 3.0), optimal_lambda\n",
        "\n",
        "    def update_agent_dual(self, agent_id, batch, epsilon_i):\n",
        "        old_policy = copy.deepcopy(self.policies[agent_id])\n",
        "        try:\n",
        "            new_mean, new_std, opt_lambda = self.solve_dual_problem(\n",
        "                agent_id, batch, old_policy, epsilon_i)\n",
        "\n",
        "            temp_policy = GaussianPolicy(new_mean, new_std)\n",
        "            w_dist = old_policy.wasserstein_distance(temp_policy)\n",
        "\n",
        "            if w_dist > epsilon_i:\n",
        "                alpha = epsilon_i / (w_dist + 1e-8)\n",
        "                new_mean = old_policy.mean + alpha * (new_mean - old_policy.mean)\n",
        "                new_std = old_policy.std + alpha * (new_std - old_policy.std)\n",
        "\n",
        "            self.policies[agent_id].update(new_mean, new_std)\n",
        "            final_w_dist = old_policy.wasserstein_distance(self.policies[agent_id])\n",
        "\n",
        "            self.lambda_history[agent_id].append(opt_lambda)\n",
        "            self.wasserstein_history[agent_id].append(final_w_dist)\n",
        "            self.epsilon_history[agent_id].append(epsilon_i)\n",
        "        except Exception as e:\n",
        "            self.policies[agent_id].update(old_policy.mean, old_policy.std)\n",
        "            self.lambda_history[agent_id].append(0.0)\n",
        "            self.wasserstein_history[agent_id].append(0.0)\n",
        "            self.epsilon_history[agent_id].append(epsilon_i)\n",
        "\n",
        "    def check_convergence(self, iteration):\n",
        "        \"\"\"Check if agents have converged to global optimum.\"\"\"\n",
        "        a1, a2 = self.policies[0].mean, self.policies[1].mean\n",
        "        distance_to_global = np.sqrt((a1 - 5.0)**2 + (a2 - 5.0)**2)\n",
        "\n",
        "        if distance_to_global < 0.5 and not self.converged_to_global:\n",
        "            self.converged_to_global = True\n",
        "            self.convergence_iteration = iteration\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def train(self):\n",
        "        basin_factor = self.env.basin_gap_factor\n",
        "        print(f\"\\nTraining OT-TRPO with basin_gap_factor={basin_factor:.1f}, method={self.adaptive_method}\")\n",
        "\n",
        "        # Print reward structure info\n",
        "        optima_info = self.env.get_reward_at_optima()\n",
        "        print(f\"  Local optimum (1,1) reward: {optima_info['local']:.3f}\")\n",
        "        print(f\"  Global optimum (5,5) reward: {optima_info['global']:.3f}\")\n",
        "        print(f\"  Reward gap: {optima_info['gap']:.3f}\")\n",
        "\n",
        "        for iteration in range(self.n_iterations):\n",
        "            batch = self.collect_batch()\n",
        "            for critic in self.critics:\n",
        "                critic.update(batch['rewards'])\n",
        "\n",
        "            adaptive_epsilons = self._get_adaptive_epsilons(batch)\n",
        "\n",
        "            for agent_id in range(self.n_agents):\n",
        "                self.update_agent_dual(agent_id, batch, adaptive_epsilons[agent_id])\n",
        "\n",
        "            avg_reward = np.mean(batch['rewards'])\n",
        "            self.reward_history.append(avg_reward)\n",
        "            self.trajectory['a1'].append(self.policies[0].mean)\n",
        "            self.trajectory['a2'].append(self.policies[1].mean)\n",
        "\n",
        "            # Check convergence\n",
        "            if self.check_convergence(iteration):\n",
        "                print(f\"  *** Converged to global optimum at iteration {iteration}! ***\")\n",
        "\n",
        "            if iteration > 0 and iteration % 500 == 0:\n",
        "                print(f\"  Iter {iteration:4d}: Reward={avg_reward:.3f}, \"\n",
        "                     f\"Actions=({self.policies[0].mean:.2f}, {self.policies[1].mean:.2f})\")\n",
        "\n",
        "        # Final status\n",
        "        if self.converged_to_global:\n",
        "            print(f\"  Final: Converged to GLOBAL optimum (iter {self.convergence_iteration})\")\n",
        "        else:\n",
        "            a1, a2 = self.policies[0].mean, self.policies[1].mean\n",
        "            if np.sqrt((a1 - 1.0)**2 + (a2 - 1.0)**2) < 0.5:\n",
        "                print(f\"  Final: Stuck at LOCAL optimum\")\n",
        "            else:\n",
        "                print(f\"  Final: Neither optimum reached\")\n",
        "\n",
        "\n",
        "def run_basin_gap_experiment():\n",
        "    \"\"\"Run experiments with different basin gap factors.\"\"\"\n",
        "\n",
        "    # Basin gap factors to test\n",
        "    basin_factors = [0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "    # Methods to test\n",
        "    methods = ['fixed', 'greedy', 'weighted', 'caatr']\n",
        "\n",
        "    # Store results\n",
        "    results = {}\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"BASIN GAP EXPERIMENT\")\n",
        "    print(\"Testing how varying the local optimum strength affects convergence\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for method in methods:\n",
        "        results[method] = {}\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Testing method: {method.upper()}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for k in basin_factors:\n",
        "            env = DifferentialGameEnv(basin_gap_factor=k)\n",
        "\n",
        "            # Set appropriate epsilon for each method\n",
        "            if method in ['greedy', 'weighted']:\n",
        "                epsilon = 0.2\n",
        "            else:\n",
        "                epsilon = 0.1\n",
        "\n",
        "            ottrpo = OTTRPO(\n",
        "                env,\n",
        "                epsilon=epsilon,\n",
        "                batch_size=30,\n",
        "                critic_lr=0.2,\n",
        "                n_iterations=3000,\n",
        "                initial_mean=1.5,\n",
        "                initial_std=0.5,\n",
        "                adaptive_method=method,\n",
        "                caatr_C=0.02 if method == 'caatr' else 0.02\n",
        "            )\n",
        "\n",
        "            ottrpo.train()\n",
        "\n",
        "            # Store results\n",
        "            results[method][k] = {\n",
        "                'converged_to_global': ottrpo.converged_to_global,\n",
        "                'convergence_iter': ottrpo.convergence_iteration,\n",
        "                'final_actions': (ottrpo.policies[0].mean, ottrpo.policies[1].mean),\n",
        "                'final_reward': ottrpo.reward_history[-1],\n",
        "                'trajectory': copy.deepcopy(ottrpo.trajectory),\n",
        "                'rewards': copy.deepcopy(ottrpo.reward_history)\n",
        "            }\n",
        "\n",
        "    # Create comparison plots\n",
        "    create_basin_gap_plots(results, basin_factors, methods)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def create_basin_gap_plots(results, basin_factors, methods):\n",
        "    \"\"\"Create visualization comparing different basin gap factors.\"\"\"\n",
        "\n",
        "    # Plot 1: Convergence success rate\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    for idx, method in enumerate(methods):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "        # Plot reward curves for different k values\n",
        "        for k in basin_factors:\n",
        "            rewards = results[method][k]['rewards']\n",
        "            label = f\"k={k:.1f}\"\n",
        "            if results[method][k]['converged_to_global']:\n",
        "                label += \" ✓\"\n",
        "            ax.plot(rewards, label=label, alpha=0.7, linewidth=2)\n",
        "\n",
        "        ax.set_title(f'{method.upper()} Method - Basin Gap Effect')\n",
        "        ax.set_xlabel('Iteration')\n",
        "        ax.set_ylabel('Average Reward')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add horizontal lines for optima\n",
        "        env_test = DifferentialGameEnv(basin_gap_factor=1.0)\n",
        "        global_reward = env_test.reward(5.0, 5.0)\n",
        "        ax.axhline(y=global_reward, color='g', linestyle='--', alpha=0.5, label='Global')\n",
        "\n",
        "    plt.suptitle('Basin Gap Experiment: Effect of Local Optimum Strength', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    plt.savefig(os.path.join('results', 'basin_gap_comparison.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Summary statistics\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Convergence success\n",
        "    ax = axes[0]\n",
        "    x = np.arange(len(basin_factors))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, method in enumerate(methods):\n",
        "        success = [1 if results[method][k]['converged_to_global'] else 0\n",
        "                  for k in basin_factors]\n",
        "        ax.bar(x + i*width, success, width, label=method.upper())\n",
        "\n",
        "    ax.set_xlabel('Basin Gap Factor (k)')\n",
        "    ax.set_ylabel('Converged to Global (1=Yes, 0=No)')\n",
        "    ax.set_title('Convergence Success by Basin Gap')\n",
        "    ax.set_xticks(x + width * 1.5)\n",
        "    ax.set_xticklabels([f\"{k:.1f}\" for k in basin_factors])\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Final rewards\n",
        "    ax = axes[1]\n",
        "    for i, method in enumerate(methods):\n",
        "        final_rewards = [results[method][k]['final_reward'] for k in basin_factors]\n",
        "        ax.plot(basin_factors, final_rewards, marker='o', label=method.upper(), linewidth=2)\n",
        "\n",
        "    ax.set_xlabel('Basin Gap Factor (k)')\n",
        "    ax.set_ylabel('Final Average Reward')\n",
        "    ax.set_title('Final Performance by Basin Gap')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('results', 'basin_gap_summary.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BASIN GAP EXPERIMENT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nConvergence to Global Optimum:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{'Method':<12} | \" + \" | \".join([f\"k={k:.1f}\" for k in basin_factors]))\n",
        "    print(\"-\" * 40)\n",
        "    for method in methods:\n",
        "        row = f\"{method.upper():<12} | \"\n",
        "        for k in basin_factors:\n",
        "            if results[method][k]['converged_to_global']:\n",
        "                row += \" ✓   | \"\n",
        "            else:\n",
        "                row += \" ✗   | \"\n",
        "        print(row)\n",
        "\n",
        "    print(\"\\nKey Insights:\")\n",
        "    print(\"- k < 1.0: Weakens local optimum, easier to reach global\")\n",
        "    print(\"- k = 1.0: Original balance between optima\")\n",
        "    print(\"- k > 1.0: Strengthens local optimum, harder to escape\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the basin gap experiment\n",
        "    results = run_basin_gap_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d1sbcJsIRvle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90633db6-c8c1-4ed5-9f40-3a042264d53a",
        "id": "-nYLhdWURvtY"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BASIN GAP EXPERIMENT\n",
            "Testing how varying the local optimum strength affects convergence\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Testing method: FIXED\n",
            "======================================================================\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=0.5, method=fixed\n",
            "  Local optimum (1,1) reward: 0.617\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: 0.413\n",
            "  Iter  500: Reward=0.613, Actions=(1.15, 1.05)\n",
            "  Iter 1000: Reward=0.619, Actions=(1.15, 1.00)\n",
            "  Iter 1500: Reward=0.614, Actions=(1.16, 1.00)\n",
            "  Iter 2000: Reward=0.619, Actions=(1.14, 1.11)\n",
            "  Iter 2500: Reward=0.620, Actions=(1.18, 1.00)\n",
            "  Final: Stuck at LOCAL optimum\n",
            "\n",
            "Training OT-TRPO with basin_gap_factor=1.0, method=fixed\n",
            "  Local optimum (1,1) reward: 1.135\n",
            "  Global optimum (5,5) reward: 1.031\n",
            "  Reward gap: -0.104\n",
            "  Iter  500: Reward=1.091, Actions=(1.08, 0.98)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hk0PxRyBRrss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
